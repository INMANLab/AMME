{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMME Data Processing\n",
    "\n",
    "This notebook loads, processes, and epochs the Emory AMME datasets...\n",
    "\n",
    "---\n",
    "> Martina Hollearn (martina.hollearn@psych.utah.edu)  \n",
    "> 05/13/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.signal import filtfilt, firwin\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit these paths for you PC:\n",
    "Please add your directories as new comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Alireza's PC (Please don't revome the comments)\n",
    "# rdDir = r'D:\\Martina Test\\Code'\n",
    "# wrDir = r'D:\\Martina Test\\Code'\n",
    "\n",
    "rdDir = r'D:\\Martina Test\\Code'\n",
    "wrDir = r'D:\\Martina Test\\Code'\n",
    "\n",
    "rdDir = os.path.normpath(rdDir)\n",
    "wrDir = os.path.normpath(wrDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function is not working now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.filter import notch_filter\n",
    "\n",
    "def notch_filter_moving_window(data, movingwin,Fs,freqs, tau = 10, mt_bandwidth = 3):\n",
    "\n",
    "    data = data.T\n",
    "    N, C  = data.shape \n",
    "    Nwin = round(Fs * movingwin[0])  # Number of samples in window\n",
    "    Nstep = round(movingwin[1] * Fs)  # Number of samples to step through\n",
    "    Noverlap = Nwin - Nstep  # Number of points in overlap\n",
    "    p = 0.05/Nwin\n",
    "\n",
    "    x = np.arange(1, Noverlap + 1)\n",
    "    smooth = 1. / (1 + np.exp(-tau * (x - Noverlap / 2) / Noverlap))  # Sigmoidal function\n",
    "    # smooth = np.tile(smooth, (C, 1)).T  # Replicate for each channel\n",
    "\n",
    "    winstart = np.arange(0, N - Nwin, Nstep)\n",
    "    nw = len(winstart)  # Number of windows\n",
    "    datafit = np.zeros((winstart[nw-1]+Nwin, C))\n",
    "\n",
    "\n",
    "    for chIdx in range(0, C):\n",
    "        for n in range(0, nw):\n",
    "            indx = slice(winstart[n], winstart[n] + Nwin)\n",
    "            datawin = data[indx, chIdx]\n",
    "\n",
    "            # Apply notch filter to the window\n",
    "            datafitwin = mne.filter.notch_filter(x = datawin, Fs = Fs, freqs = freqs, \n",
    "                                                method='spectrum_fit', mt_bandwidth=mt_bandwidth, \n",
    "                                                p_value = p, verbose='WARNING')\n",
    "            if n==0:\n",
    "                datafitwin0 = datafitwin.copy()    \n",
    "            if n > 0:\n",
    "                # Apply smoothing effect to the overlapping region\n",
    "                datafitwinTemp = datafitwin.copy()\n",
    "                datafitwin[:Noverlap] = smooth * datafitwin[:Noverlap] + (1 - smooth) * datafitwin0[Nwin - Noverlap:Nwin]\n",
    "                datafitwin0 = datafitwinTemp.copy()\n",
    "\n",
    "            datafit[indx, chIdx] = datafitwin\n",
    "\n",
    "    return datafit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure that the file names for each files called in match. Filenames are inconsistent throughtout the AMME dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'amyg003'\n",
    "seeg_filename = f'{subject}_objectMemory_day2_05mA.edf'\n",
    "file_path = rdDir\n",
    "event_filename = f'{subject}_LFP_day2_trialtimes.mat'\n",
    "log_filename = f'{subject}_day2.log'\n",
    "data_path = os.path.join(file_path, subject, seeg_filename)\n",
    "events_path = os.path.join(file_path, subject, event_filename)\n",
    "logfile_path = os.path.join(file_path, subject, log_filename)\n",
    "\n",
    "# Create Preprocessed data folder\n",
    "preproc_datapath = os.path.join(file_path,subject,'PreprocessedData', 'Martinas_preprocessing')\n",
    "\n",
    "if not os.path.exists(preproc_datapath):\n",
    "    os.makedirs(preproc_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SEEG data\n",
    "raw = mne.io.read_raw_edf(data_path, preload=True)\n",
    "\n",
    "# Get recording info\n",
    "fs = int(np.round(raw.info['sfreq']))\n",
    "ch_names = raw.info['ch_names']\n",
    "\n",
    "# Set all channel types to SEEG\n",
    "raw.set_channel_types({ch: 'seeg' for ch in ch_names})\n",
    "\n",
    "# Display info\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and compare notch filter\n",
    "# Define your parameters\n",
    "lowcut = 1  # Lower cutoff frequency\n",
    "highcut = 119  # Upper cutoff frequency\n",
    "transition_bandwidth = 1  # Transition bandwidth in Hz\n",
    "\n",
    "# Dynamically get the subject's sampling rate\n",
    "samprate = raw.info['sfreq']  # This pulls the sampling rate for the current subject\n",
    "\n",
    "#Create a Copy and filter the data\n",
    "proc_data = raw.copy()\n",
    "\n",
    "data = proc_data.get_data()\n",
    "data = notch_filter_moving_window(data = data[31:33,:], movingwin=[1.5, .5], Fs=samprate, freqs=[60], tau=10, mt_bandwidth=3)\n",
    "data = notch_filter_moving_window(data = data.T, movingwin=[2, 1], Fs=samprate, freqs=[42], tau=10, mt_bandwidth=3)\n",
    "\n",
    "print(data.shape)\n",
    "savemat(os.path.join(wrDir,'NotchFilterTest.mat'), {'eeg_data': data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "savemat(os.path.join(wrDir,'NotchFilterTest.mat'), {'eeg_data': data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Preprocessing steps\n",
    "- Extract events (trial times, stim types, responses)\n",
    "- Filtering (lowpass-, highpass-, and notch w harmonics)\n",
    "- Data cleaning by identifying bad channels and epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters\n",
    "lowcut = 1  # Lower cutoff frequency\n",
    "highcut = 119  # Upper cutoff frequency\n",
    "transition_bandwidth = 1  # Transition bandwidth in Hz\n",
    "\n",
    "# Dynamically get the subject's sampling rate\n",
    "samprate = raw.info['sfreq']  # This pulls the sampling rate for the current subject\n",
    "\n",
    "#Create a Copy and filter the data\n",
    "proc_data = raw.copy()\n",
    "proc_data.filter(lowcut, highcut, fir_design='firwin', filter_length='auto', phase='zero', fir_window='hamming', \n",
    "                 verbose=True,l_trans_bandwidth=1, h_trans_bandwidth=1)\n",
    "\n",
    "#filter out 42 Hz noise\n",
    "proc_data.notch_filter(freqs = [42], picks='seeg', method='spectrum_fit', mt_bandwidth=3,verbose=True)# Joe had this in his code, not sure why it's there\n",
    "\n",
    "# Apply notch filter (60, 120, 180 Hz)\n",
    "proc_data.notch_filter(freqs = [60], picks='seeg', method='spectrum_fit', mt_bandwidth=3,verbose=True)\n",
    "\n",
    "savemat(os.path.join(wrDir,'PythonPreprocessed.mat'), {'eeg_data': proc_data.get_data()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Plot Filter vs Raw data to ensure that the filtering works\n",
    "### accoding to Joe it works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the filtered data\n",
    "filtered_data = proc_data.get_data(picks='seeg')\n",
    "\n",
    "# Check if there are any NaN values in the data\n",
    "nan_count = np.isnan(filtered_data).sum()\n",
    "print(f\"Number of NaN values: {nan_count}\")\n",
    "\n",
    "# Check if there are any Inf values in the data\n",
    "inf_count = np.isinf(filtered_data).sum()\n",
    "print(f\"Number of Inf values: {inf_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace NaN or Inf values with 0\n",
    "def clean_data(data):\n",
    "    return np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Clean the data for raw, bandpass, and bandpass + notch\n",
    "raw_data_clean = clean_data(raw.get_data())\n",
    "bandpass_data_clean = clean_data(bandpass.get_data())\n",
    "bandpass_and_notch_data_clean = clean_data(proc_data.get_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust here\n",
    "time_start = 0  # in ms\n",
    "time_end = 200  # in ms\n",
    "###\n",
    "\n",
    "# Convert time from ms to sample indices\n",
    "time_start_idx = int(np.round(time_start * fs / 1000))  # Start sample index\n",
    "time_end_idx = int(np.round(time_end * fs / 1000))      # End sample index\n",
    "print(f'Time range: {time_start_idx} to {time_end_idx} samples')\n",
    "\n",
    "# Clean the data to avoid NaN or Inf values\n",
    "raw_data_clean = clean_data(raw.get_data())\n",
    "bandpass_data_clean = clean_data(bandpass.get_data())\n",
    "bandpass_and_notch_data_clean = clean_data(proc_data.get_data())\n",
    "\n",
    "# Set up saving path\n",
    "raw_vs_filter_path = os.path.join(preproc_datapath, 'raw_vs_filter_plots', f'{time_start}ms_to_{time_end}ms')\n",
    "os.makedirs(raw_vs_filter_path, exist_ok=True)\n",
    "\n",
    "num_channels = raw_data_clean.shape[0]\n",
    "time_range = np.arange(time_start_idx, time_end_idx)  # X-axis for ms\n",
    "\n",
    "# Time range in samples for X-axis\n",
    "time_range = np.arange(time_start_idx, time_end_idx)\n",
    "\n",
    "for channel in range(num_channels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot raw, bandpass, and bandpass + notch filtered data\n",
    "    plt.plot(time_range, raw_data_clean[channel, time_start_idx:time_end_idx], label='Raw')\n",
    "    plt.plot(time_range, bandpass_data_clean[channel, time_start_idx:time_end_idx], label='Bandpass')\n",
    "    plt.plot(time_range, bandpass_and_notch_data_clean[channel, time_start_idx:time_end_idx], label='Bandpass + Notch')\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.title(f'{subject} Channel {channel + 1} ({raw.ch_names[channel]}) from {time_start}ms to {time_end}ms')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Amplitude (µV)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Adjust y-axis limits dynamically based on the cleaned data\n",
    "    y_min = min(np.min(raw_data_clean[channel, time_start_idx:time_end_idx]),\n",
    "                np.min(bandpass_data_clean[channel, time_start_idx:time_end_idx]),\n",
    "                np.min(bandpass_and_notch_data_clean[channel, time_start_idx:time_end_idx]))\n",
    "    y_max = max(np.max(raw_data_clean[channel, time_start_idx:time_end_idx]),\n",
    "                np.max(bandpass_data_clean[channel, time_start_idx:time_end_idx]),\n",
    "                np.max(bandpass_and_notch_data_clean[channel, time_start_idx:time_end_idx]))\n",
    "\n",
    "    if np.isfinite(y_min) and np.isfinite(y_max):\n",
    "        plt.ylim([y_min - abs(y_min * 0.1), y_max + abs(y_max * 0.1)])  # Add 10% margin\n",
    "    else:\n",
    "        plt.ylim([-100, 100])  # Fallback y-axis limit\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(os.path.join(raw_vs_filter_path, f'channel_{channel + 1}.png'))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjust here\n",
    "time_start = 0 # in ms\n",
    "time_end = 100 # in ms\n",
    "###\n",
    "\n",
    "# Convert time from ms to sample indices\n",
    "time_start_idx = int(np.round(time_start * fs / 1000))  # Start sample index\n",
    "time_end_idx = int(np.round(time_end * fs / 1000))      # End sample index\n",
    "print(f'Time range: {time_start_idx} to {time_end_idx} samples')\n",
    "\n",
    "raw_data = raw.get_data()\n",
    "bandpass_data = bandpass.get_data()\n",
    "bandpass_and_notch_data = proc_data.get_data()\n",
    "\n",
    "# Set up saving path\n",
    "raw_vs_filter_path = os.path.join(preproc_datapath, 'raw_vs_filter_plots', f'{time_start}ms_to_{time_end}ms')\n",
    "os.makedirs(raw_vs_filter_path, exist_ok=True)\n",
    "\n",
    "num_channels = raw_data.shape[0]\n",
    "time_range = np.arange(time_start_idx, time_end_idx)  # X-axis for ms\n",
    "\n",
    "\n",
    "for channel in range(num_channels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(time_range, raw_data[channel, time_start_idx:time_end_idx], label='Raw')\n",
    "    plt.plot(time_range, bandpass_data[channel, time_start_idx:time_end_idx], label='Bandpass')\n",
    "    plt.plot(time_range, bandpass_and_notch_data[channel, time_start_idx:time_end_idx], label='Bandpass + Notch')\n",
    "\n",
    "    # Adding labels and title\n",
    "    plt.title(f'{subject} Channel {channel + 1} ({raw.ch_names[channel]}) from {time_start}ms to {time_end}ms')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Amplitude (uV)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(raw_vs_filter_path, f'channel_{channel + 1}.png'))\n",
    "    #plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data visually\n",
    "\n",
    "1. Mark in the interactive plot all bad channels, then remove them\n",
    "2. Mark in the interactive plot all bad epochs by annotations, then remove them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If available, load in dropped channels from DroppedChans.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot filtered data with event markers colored by event types\n",
    "proc_data.plot(title='Filtered EEG Data', block=True, clipping = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call in previously dropped channels\n",
    "file_path = os.path.join(preproc_datapath,'DroppedChans.csv')\n",
    "dropped_chans = []\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        dropped_chans.append(row[1])\n",
    "       \n",
    "dropped_chans = dropped_chans[1:]\n",
    "dropped_chans = list(filter(None, dropped_chans))  # Remove empty strings\n",
    "\n",
    "print(dropped_chans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop channels (hand-picked or from previous analysis)\n",
    "bads =['Event', 'L25d9', 'LSPs7', 'C128', 'C127', 'C126', 'C125', 'C123', 'C124', 'C122', 'C121', 'C120', 'C119', 'L17d1', 'L17d7', 'L13d5'] \n",
    "\n",
    "proc_data = proc_data.drop_channels(bads)\n",
    "proc_data.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post Cleaning Preprocessing Steps\n",
    "- Re-referencing (e.g., common median reference)\n",
    "- Downsampling\n",
    "- Epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-referencing\n",
    "proc_data_ref = proc_data.get_data()  # Convert to numpy array\n",
    "median_lfp = np.median(proc_data_ref, axis=0)  # Calculate median LFP\n",
    "proc_data_ref = proc_data_ref - median_lfp  # Subtract median LFP for re-referencing\n",
    "proc_data = mne.io.RawArray(proc_data_ref, proc_data.info)  # Convert back to MNE object\n",
    "\n",
    "# Downsampling\n",
    "fs = 500\n",
    "proc_data = proc_data.resample(sfreq=fs)\n",
    "\n",
    "# Plot filtered data with event markers colored by event types\n",
    "#proc_data.plot(title='Filtered EEG Data', block=True, clipping=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the log file and filter by stimulation types to find new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load logfile\n",
    "logfile = pd.read_csv(logfile_path, delimiter='\\t', skiprows=2, skipfooter=1) #tab delimited csv file is our log file format, reject first 2 rows and the last row\n",
    "\n",
    "# Filter out the NaN or New response condition rows\n",
    "#enumerate stim categories from log files to numbers\n",
    "#ORIGINAL\n",
    "logfile.loc[logfile['CONDITION'] == 'nostim', 'CONDITION'] = 0\n",
    "logfile.loc[logfile['CONDITION'] == 'stim', 'CONDITION'] = 1\n",
    "logfile.loc[logfile['CONDITION'] == 'new', 'CONDITION'] = 9\n",
    "logfile.loc[logfile['CONDITION'] == 'None', 'CONDITION'] = np.nan\n",
    "\n",
    "# Identify NaN rows\n",
    "dropped_nan_indices = logfile[logfile['CONDITION'].isna()].index\n",
    "print(f'Indices of NaN rows: {dropped_nan_indices.tolist()}')\n",
    "\n",
    "# Drop NaN rows and reorder the index\n",
    "logfile = logfile.dropna(subset=['CONDITION']).reset_index(drop=True)\n",
    "\n",
    "# Save CONDITION column to a .npy file\n",
    "np.save(os.path.join(preproc_datapath, subject + '_stimcondition'), logfile['CONDITION'].values)\n",
    "np.save(os.path.join(preproc_datapath, subject + '_dropped_nan_indices'), dropped_nan_indices)#save dropped nan indices\n",
    "\n",
    "# Test loading the saved file\n",
    "test = np.load(os.path.join(preproc_datapath, subject + '_stimcondition.npy'), allow_pickle=True)\n",
    "print('length of stim condition',len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the same log file find response types and perform Signal Detection analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract responses from log file for remembered vs forgotten analysis\n",
    "logfile.loc[logfile['YES/NO'] == 'yes', 'YES/NO'] = 1 # 1 for yes\n",
    "logfile.loc[logfile['YES/NO'] == 'no', 'YES/NO'] = 0 # 0 for no\n",
    "\n",
    "# Ensure no NaN values in YES/NO column\n",
    "logfile['YES/NO'] = pd.to_numeric(logfile['YES/NO'], errors='coerce')\n",
    "logfile = logfile.dropna(subset=['YES/NO']).reset_index(drop=True)\n",
    "logfile['YES/NO'] = logfile['YES/NO'].astype(int) #convert to integer\n",
    "\n",
    "# Calculate remembered vs forgotten data from responses: from 'Condition' column we can extract image condition as 'new' and anything that's not 'new' as 'target'\n",
    "hit = logfile[(logfile['CONDITION']!=9) & (logfile['YES/NO'] == 1)]\n",
    "miss = logfile[(logfile['CONDITION']!=9) & (logfile['YES/NO'] == 0)]\n",
    "fa= logfile[(logfile['CONDITION'] ==9) & (logfile['YES/NO'] == 1)] \n",
    "cr = logfile[(logfile['CONDITION'] ==9) & (logfile['YES/NO'] == 0)]\n",
    "\n",
    "print('hit:', hit.shape)\n",
    "print('miss:', miss.shape)\n",
    "print('fa:', fa.shape)\n",
    "print('cr:', cr.shape)\n",
    "\n",
    "nhits = hit.shape[0]\n",
    "nmiss = miss.shape[0]\n",
    "nfa= fa.shape[0] # -----> check this with a new subject bc we got 40/40 FA/CR for amyg030\n",
    "ncr = cr.shape[0]\n",
    "\n",
    "remembered = hit # where the subject responded YES,and accurately recognized the image\n",
    "forgotten = miss # where the subject responded NO, and did NOT accurately recognize the image\n",
    "\n",
    "n_remembered = remembered.shape[0]\n",
    "n_forgotten = forgotten.shape[0]\n",
    "\n",
    "hitrate = nhits/(nhits+nmiss)\n",
    "hits_index = hit.index\n",
    "miss_index = miss.index\n",
    "cr_index = cr.index\n",
    "fa_index = fa.index\n",
    "\n",
    "print(\"remembered: \", n_remembered)\n",
    "print(\"forgotten: \", n_forgotten)\n",
    "print(\"fa:\", nfa)\n",
    "print(\"cr:\", ncr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Signal Detection data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe to store the indices and their corresponding response types\n",
    "response_data = pd.DataFrame({\n",
    "    'Index': hit.index.tolist() + miss.index.tolist() + fa.index.tolist() + cr.index.tolist(),\n",
    "    'Response': ['hit'] * len(hit.index) + ['miss'] * len(miss.index) + ['fa'] * len(fa.index) + ['cr'] * len(cr.index)\n",
    "})\n",
    "\n",
    "# Sort the dataframe by the index to maintain the order of the original logfile\n",
    "response_data = response_data.sort_values(by='Index').reset_index(drop=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "response_data.to_csv(os.path.join(preproc_datapath, subject + '_SignalDetection_ResponseData.csv'))\n",
    "\n",
    "#Write signal detection ratios into a txt file\n",
    "with open(os.path.join(preproc_datapath, subject + '_SignalDetection_Ratios_Before_Dropping_Epochs.txt'), 'w') as f:\n",
    "    f.write(f'Hits: {nhits}\\n')\n",
    "    f.write(f'Misses: {nmiss}\\n')\n",
    "    f.write(f'False Alarms: {nfa}\\n')\n",
    "    f.write(f'Correct Rejections: {ncr}\\n')\n",
    "    f.write(f'Hit Rate: {hitrate}\\n')\n",
    "    f.write(f'Total Trials: {nhits + nmiss + nfa + ncr}\\n')\n",
    "    f.write(f'Remembered: {n_remembered}\\n')\n",
    "    f.write(f'Forgotten: {n_forgotten}\\n')\n",
    "    f.write(f'Hit Rate: {hitrate}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After filtering the logfile, drop rows to keep only remembered and forgotten items (no new items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of the original indices before dropping rows\n",
    "original_indices = logfile.index.to_list()\n",
    "print(\"Original length of rows:\",len(original_indices))\n",
    "\n",
    "#Drop rows where condition is 'new', so we can analyze remembered vs forgotten\n",
    "logfile = logfile[logfile['CONDITION']!=9].reset_index(drop=True)\n",
    "print(\"New number of rows:\", logfile.shape[0])\n",
    "print(logfile['YES/NO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in trial times from matlab file, filter by response types for remembered and forgotten images (filter out new images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in event times from mat file, these are the trial times in seconds\n",
    "events = loadmat(events_path, simplify_cells=True)\n",
    "day2_trial_times = events['day2_trial_times']\n",
    "day2_trial_times = day2_trial_times * fs # convert to samples\n",
    "day2_trial_times = np.array([int(np.round(x)) for x in day2_trial_times]) # round to nearest integer\n",
    "print('lenght of day2_trial_times:',len(day2_trial_times))\n",
    "print(\"length of dropped nan indices:\",len(dropped_nan_indices))\n",
    "print(dropped_nan_indices.shape)   \n",
    "\n",
    "# Determine the indices to drop in day2_trial_times\n",
    "indices_to_drop = list(fa.index) + list(cr.index)\n",
    "print(\"Number of rows to drop:\",len(indices_to_drop))\n",
    "\n",
    "# Drop the corresponding trial times\n",
    "day2_trial_times = np.delete(day2_trial_times, indices_to_drop)\n",
    "print(\"Length of FA trials:\", len(fa.index), \"FA indices:\", fa.index)\n",
    "print(\"Length of CR trials:\", len(cr.index),\"CR indices:\", cr.index)\n",
    "\n",
    "# Verify the indices match between day2_trial_times and logfile\n",
    "assert len(day2_trial_times) == logfile.shape[0], \"Mismatch between day2_trial_times and logfile\"\n",
    "\n",
    "# Now, day2_trial_times should match the filtered logfile indices\n",
    "print(\"Indices match successfully!\")\n",
    "print(\"Remaining length of day2_trial_times:\", len(day2_trial_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create event array based on response types (remembered vs forgotten)\n",
    "n_events = len(day2_trial_times)\n",
    "(print(n_events))\n",
    "events_array = np.zeros((n_events, 3), dtype=int)\n",
    "print(events_array.shape)\n",
    "events_array[:, 0] = day2_trial_times\n",
    "print(logfile['YES/NO'].values)\n",
    "print(len(logfile['YES/NO'].values))\n",
    "\n",
    "# Count the number of zeros\n",
    "zero_count = np.count_nonzero(logfile['YES/NO'].values ==0)\n",
    "print(\"Number of zeros:\", zero_count)\n",
    "events_array[:, 2] = logfile['YES/NO'].values # set event IDs here based on log file info\n",
    "\n",
    "#Check if events_array[3] is the same as logfile['YES/NO']\n",
    "if events_array[:, 2].all() == logfile['YES/NO'].values.all():\n",
    "    print(\"Event IDs match successfully!\")\n",
    "else:\n",
    "    print(\"Event IDs do not match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch data based on response types with events marking remembered (1) versus forgotten (0) items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epoching\n",
    "epochs = mne.Epochs(proc_data, events_array, tmin = -5, tmax = 5, baseline = None, reject=None) # 5s before to 5s after event onset\n",
    "epochs.plot(title='Epoched EEG Data', block=True, events=events_array)\n",
    "\n",
    "print('num events',events_array.shape[0])\n",
    "print('num epochs',len(epochs))\n",
    "\n",
    "# Drop epochs if needed which will also drop that trial\n",
    "epochs.info['bads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export the data into numpy arrays for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get raw data, list of rejected epochs, list of bad_chans\n",
    "epoch_data = epochs.get_data()\n",
    "drop_epochs = [n for n, dl in enumerate(epochs.drop_log) if len(dl)]\n",
    "events_mask = np.ones(events_array.shape[0], dtype = bool)\n",
    "events_mask[drop_epochs] = False #drop epochs from events array\n",
    "keep_events = events_array[events_mask] \n",
    "\n",
    "# Export dropped epochs, dropped chans, events, and channel labels to .csv files\n",
    "np.save(os.path.join(preproc_datapath, ('PreprocessedData')), epoch_data)\n",
    "np.save(os.path.join(preproc_datapath, ('Events')), keep_events) #saves only the epochs that were not dropped manually\n",
    "pd.DataFrame(drop_epochs, columns = ['Dropped Epochs']).to_csv(os.path.join(preproc_datapath,'DroppedEpochs.csv'))\n",
    "pd.DataFrame(bads, columns = ['Dropped Chans']).to_csv(os.path.join(preproc_datapath,'DroppedChans.csv'))\n",
    "pd.DataFrame(epochs.ch_names, columns = ['Chan']).to_csv(os.path.join(preproc_datapath,'ChanLabels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(os.path.join(preproc_datapath, ('Events.npy')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
